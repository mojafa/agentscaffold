"""Agent implementation for {{agent_name}}."""

import os
import json
import inspect
from typing import Dict, Any, Optional, List, ClassVar, Type

from pydantic import BaseModel, Field, ConfigDict

# Try to import from agentscaffold, but provide fallbacks if not available
try:
    from agentscaffold.agent import Agent as BaseAgent, AgentInput, AgentOutput, DaytonaRuntime
    from pydantic_ai import Agent as PydanticAgent
except ImportError:
    # Fallback if agentscaffold is not available
    class AgentInput(BaseModel):
        """Base class for agent inputs."""
        message: str = Field(..., description="Input message for the agent")
        context: Dict[str, Any] = Field(default_factory=dict, description="Additional context")

    class AgentOutput(BaseModel):
        """Base class for agent outputs."""
        response: str = Field(..., description="Response from the agent")
        metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

    class DaytonaRuntime:
        """Minimal DaytonaRuntime implementation."""
        async def execute(self, agent_dir, input_data):
            return {
                "response": f"Daytona execution failed: agentscaffold not available.",
                "metadata": {"error": True}
            }

    class BaseAgent(BaseModel):
        """Minimal base agent implementation."""
        model_config = ConfigDict(arbitrary_types_allowed=True)
        
        name: str = "MinimalAgent"
        description: str = ""
        runtime: Optional[Any] = None
        
        def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """Process input data and generate output."""
            return {"response": f"Processed: {input_data['message']}", "metadata": {}}
        
        async def run(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """Run the agent with the provided input data."""
            if isinstance(input_data, str):
                input_data = {"message": input_data}
                
            # Create a simple validated input
            validated_input = {"message": input_data.get("message", ""), "context": input_data.get("context", {})}
            
            # Process the input
            result = self.process(validated_input)
            
            # Return the result as a dict
            return result
    
    # Fallback minimal PydanticAgent
    class PydanticAgent:
        """Minimal PydanticAgent implementation when pydantic_ai is not available."""
        
        def __init__(self, model_name, result_type=None, system_prompt=None):
            self.model_name = model_name
            self.result_type = result_type
            self.system_prompt = system_prompt
            print(f"Warning: Using fallback PydanticAgent. Install pydantic_ai for full functionality.")
        
        async def run(self, message):
            """Run the agent with the provided message."""
            try:
                # Try to import a direct LLM client based on model_name prefix
                if self.model_name.startswith("openai:"):
                    # Try OpenAI
                    import openai
                    client = openai.OpenAI()
                    response = client.chat.completions.create(
                        model=self.model_name.split(":", 1)[1],
                        messages=[
                            {"role": "system", "content": self.system_prompt or "You are a helpful assistant."},
                            {"role": "user", "content": message}
                        ]
                    )
                    result_text = response.choices[0].message.content
                elif self.model_name.startswith("anthropic:"):
                    # Try Anthropic
                    import anthropic
                    client = anthropic.Anthropic()
                    response = client.messages.create(
                        model=self.model_name.split(":", 1)[1],
                        system=self.system_prompt or "You are a helpful assistant.",
                        messages=[{"role": "user", "content": message}]
                    )
                    result_text = response.content[0].text
                else:
                    # Fallback response
                    result_text = f"I received your message: {message}. However, I'm running in fallback mode without LLM access."
                
                # Create the result object
                result_data = {"message": result_text, "additional_info": {}}
                
                # Create a simple result class
                class MinimalResult:
                    def __init__(self, data):
                        self.data = type('ResultData', (), data)
                
                return MinimalResult(result_data)
            except Exception as e:
                print(f"Error in fallback PydanticAgent: {str(e)}")
                # Even more minimal fallback
                result_data = {"message": f"I received your message, but encountered an error: {str(e)}", "additional_info": {"error": True}}
                class MinimalResult:
                    def __init__(self, data):
                        self.data = type('ResultData', (), data)
                return MinimalResult(result_data)


class {{agent_class_name}}Input(AgentInput):
    """Input for {{agent_class_name}} agent."""
    
    # Add custom input fields here
    {% if search_provider != "none" %}
    search_query: Optional[str] = Field(None, description="Optional search query")
    {% endif %}
    {% if memory_provider != "none" %}
    retrieve_context: bool = Field(default=False, description="Whether to retrieve context from memory")
    context_query: Optional[str] = Field(None, description="Optional query for retrieving context")
    {% endif %}
    pass


class {{agent_class_name}}Output(AgentOutput):
    """Output for {{agent_class_name}} agent."""
    
    # Add custom output fields here
    {% if search_provider != "none" %}
    search_results: Optional[List[Dict[str, Any]]] = Field(None, description="Search results if any")
    {% endif %}
    {% if memory_provider != "none" %}
    context: Optional[List[Dict[str, Any]]] = Field(None, description="Retrieved context if any")
    {% endif %}
    pass


class {{agent_class_name}}PydanticResult(BaseModel):
    """Result from Pydantic AI Agent."""
    
    message: str = Field(description="Response message")
    additional_info: Dict[str, Any] = Field(default_factory=dict, description="Additional information")


class Agent(BaseAgent):
    """{{agent_class_name}} agent implementation."""
    
    # Use ConfigDict to set model config
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    name: str = "{{agent_class_name}}"
    description: str = "A {{agent_name}} agent"
    
    # Properly annotate class variables
    input_class: ClassVar[Type[AgentInput]] = {{agent_class_name}}Input
    output_class: ClassVar[Type[AgentOutput]] = {{agent_class_name}}Output
    
    def __init__(self, **data):
        super().__init__(**data)
            
        # Initialize Pydantic AI agent
        try:
            self.pydantic_agent = PydanticAgent(
                {% if llm_provider == "openai" %}
                "openai:gpt-4o",  # Can be configured via environment variables
                {% elif llm_provider == "anthropic" %}
                "anthropic:claude-3-opus-20240229",  # Can be configured via environment variables
                {% elif llm_provider == "daytona" %}
                "daytona:openai/gpt-4o",  # Can be configured via environment variables
                {% elif llm_provider == "huggingface" %}
                "huggingface:mistralai/Mistral-7B-Instruct-v0.2",  # Can be configured via environment variables
                {% elif llm_provider == "ollama" %}
                "ollama:llama3",  # Can be configured via environment variables
                {% else %}
                "openai:gpt-4o",  # Can be configured via environment variables
                {% endif %}
                result_type={{agent_class_name}}PydanticResult,
                system_prompt=(
                    "You are {{agent_class_name}}, an AI assistant designed to help with "
                    "{{agent_name}}. Be helpful, concise, and accurate in your responses."
                )
            )
        except Exception as e:
            print(f"Warning: Error initializing PydanticAgent: {str(e)}")
            # Create a simple fallback agent that just echoes messages
            self.pydantic_agent = PydanticAgent("fallback", result_type={{agent_class_name}}PydanticResult)
        
        {% if search_provider != "none" %}
        # Initialize search provider
        self.search = self._init_search()
        {% endif %}
        
        {% if memory_provider != "none" %}
        # Initialize memory provider
        self.memory = self._init_memory()
        {% endif %}
        
        {% if logging_provider != "none" %}
        # Initialize logging provider
        self.logging = self._init_logging()
        {% endif %}
    
    {% if search_provider != "none" %}
    def _init_search(self):
        """Initialize search provider."""
        try:
            {% if search_provider == "brave" %}
            from brave_search import BraveSearch
            api_key = os.getenv("BRAVE_API_KEY")
            if api_key:
                return BraveSearch(api_key=api_key)
            {% elif search_provider == "browserbase" %}
            from browserbase import BrowserBase
            api_key = os.getenv("BROWSERBASE_API_KEY")
            if api_key:
                return BrowserBase(api_key=api_key)
            {% endif %}
        except ImportError:
            print(f"Warning: {search_provider} package not installed. Search functionality disabled.")
        return None
    
    async def search_web(self, query: str) -> List[Dict[str, Any]]:
        """Search the web for information."""
        if not self.search or not query:
            return []
        
        try:
            {% if search_provider == "brave" %}
            results = await self.search.search(query, count=5)
            return [
                {"title": r.title, "url": r.url, "description": r.description}
                for r in results.web.results
            ]
            {% elif search_provider == "browserbase" %}
            results = await self.search.search(query, limit=5)
            return [
                {"title": r.title, "url": r.url, "description": r.snippet}
                for r in results
            ]
            {% endif %}
        except Exception as e:
            print(f"Search error: {str(e)}")
            return []
    {% endif %}
    
    {% if memory_provider != "none" %}
    def _init_memory(self):
        """Initialize memory provider."""
        try:
            {% if memory_provider == "supabase" %}
            from supabase import create_client
            url = os.getenv("SUPABASE_URL")
            key = os.getenv("SUPABASE_KEY")
            if url and key:
                return create_client(url, key)
            {% elif memory_provider == "milvus" %}
            from pymilvus import connections, Collection
            uri = os.getenv("MILVUS_URI")
            if uri:
                connections.connect(uri=uri)
                return Collection("agent_memory")
            {% elif memory_provider == "chromadb" %}
            import chromadb
            db_path = os.getenv("CHROMA_DB_PATH", "./chroma_db")
            client = chromadb.PersistentClient(path=db_path)
            return client.get_or_create_collection("agent_memory")
            {% endif %}
        except ImportError:
            print(f"Warning: {memory_provider} package not installed. Memory functionality disabled.")
        return None
    
    async def retrieve_from_memory(self, query: str) -> List[Dict[str, Any]]:
        """Retrieve context from memory."""
        if not self.memory or not query:
            return []
        
        try:
            {% if memory_provider == "supabase" %}
            # Using pgvector via Supabase
            # This assumes you have a properly set up table and embedding function
            embedding = await self._get_embedding(query)
            result = self.memory.table("memories").select("*").execute()
            return result.data
            {% elif memory_provider == "milvus" %}
            # Using Milvus
            embedding = await self._get_embedding(query)
            search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
            results = self.memory.search(
                data=[embedding],
                anns_field="embedding",
                param=search_params,
                limit=5,
                output_fields=["text", "metadata"],
            )
            return [hit.__dict__ for hit in results[0]]
            {% elif memory_provider == "chromadb" %}
            # Using ChromaDB
            results = self.memory.query(
                query_texts=[query],
                n_results=5,
            )
            items = []
            for i in range(len(results["ids"][0])):
                items.append({
                    "id": results["ids"][0][i],
                    "text": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i],
                })
            return items
            {% endif %}
        except Exception as e:
            print(f"Memory retrieval error: {str(e)}")
            return []
    
    async def _get_embedding(self, text: str) -> List[float]:
        """Get embedding for text using OpenAI or local alternative."""
        try:
            import openai
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text,
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding error: {str(e)}")
            # Return a zero vector as fallback (not ideal but prevents crashes)
            return [0.0] * 1536
    {% endif %}
    
    {% if logging_provider != "none" %}
    def _init_logging(self):
        """Initialize logging provider."""
        try:
            {% if logging_provider == "logfire" %}
            import logfire
            api_key = os.getenv("LOGFIRE_API_KEY")
            if api_key:
                logfire.init(api_key=api_key)
                return logfire
            {% endif %}
        except ImportError:
            print(f"Warning: {logging_provider} package not installed. Logging functionality disabled.")
        return None
    
    def log_event(self, event_name: str, **kwargs):
        """Log an event with the logging provider."""
        if self.logging:
            try:
                self.logging.info(event_name, **kwargs)
            except Exception as e:
                print(f"Logging error: {str(e)}")
    {% endif %}
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input data and generate output.
        
        Args:
            input_data: Validated input data
            
        Returns:
            Agent output
        """
        try:
            metadata = {}
            
            {% if logging_provider != "none" %}
            # Log the request
            self.log_event("agent_request", input=input_data["message"])
            {% endif %}
            
            {% if search_provider != "none" %}
            # Handle search if requested
            search_results = []
            search_query = input_data.get("context", {}).get("search_query")
            if search_query:
                {% if logging_provider != "none" %}
                self.log_event("search_request", query=search_query)
                {% endif %}
                search_results = await self.search_web(search_query)
                metadata["search_results"] = search_results
            {% endif %}
            
            {% if memory_provider != "none" %}
            # Handle memory retrieval if requested
            context = []
            if input_data.get("context", {}).get("retrieve_context", False):
                context_query = input_data.get("context", {}).get("context_query", input_data["message"])
                {% if logging_provider != "none" %}
                self.log_event("memory_request", query=context_query)
                {% endif %}
                context = await self.retrieve_from_memory(context_query)
                metadata["context"] = context
            {% endif %}
            
            # Build enhanced prompt with context and search results
            enhanced_message = input_data["message"]
            
            {% if search_provider != "none" or memory_provider != "none" %}
            context_additions = []
            
            {% if search_provider != "none" %}
            if search_results:
                search_context = "\n\nSearch Results:\n"
                for i, result in enumerate(search_results[:3]):  # Limit to top 3 results
                    search_context += f"{i+1}. {result['title']} - {result['url']}\n{result['description']}\n"
                context_additions.append(search_context)
            {% endif %}
            
            {% if memory_provider != "none" %}
            if context:
                memory_context = "\n\nRelevant Context:\n"
                for i, item in enumerate(context[:3]):  # Limit to top 3 items
                    if "text" in item:
                        memory_context += f"{i+1}. {item['text']}\n"
                    elif "content" in item:
                        memory_context += f"{i+1}. {item['content']}\n"
                context_additions.append(memory_context)
            {% endif %}
            
            if context_additions:
                enhanced_message = enhanced_message + "\n\n" + "\n".join(context_additions)
            {% endif %}
            
            # Process with Pydantic AI
            try:
                result = await self.pydantic_agent.run(enhanced_message)
                response_message = result.data.message
                additional_info = result.data.additional_info
            except Exception as e:
                print(f"Error in pydantic_agent.run: {str(e)}")
                # Fallback response
                response_message = f"I received your message: {input_data['message']}. However, I encountered an error while processing."
                additional_info = {"error": str(e)}
            
            {% if logging_provider != "none" %}
            # Log the response
            self.log_event("agent_response", response=response_message)
            {% endif %}
            
            # Return response with metadata
            response = {
                "response": response_message,
                "metadata": {**additional_info, **metadata}
            }
            
            {% if search_provider != "none" %}
            # Include search results in the output if available
            if search_results:
                response["search_results"] = search_results
            {% endif %}
            
            {% if memory_provider != "none" %}
            # Include context in the output if available
            if context:
                response["context"] = context
            {% endif %}
            
            return response
            
        except Exception as e:
            {% if logging_provider != "none" %}
            # Log the error
            self.log_event("agent_error", error=str(e))
            {% endif %}
            
            # Handle any errors
            return {
                "response": f"Error: {str(e)}",
                "metadata": {"error": True}
            }