"""Agent implementation for {{agent_name}}."""

import os
import time
import json
import asyncio
from typing import Dict, Any, List, Optional, ClassVar, Type, Union

from pydantic import BaseModel, Field, ConfigDict

# Try to import from agentscaffold, but provide fallbacks if not available
try:
    from agentscaffold.agent import Agent as BaseAgent, AgentInput, AgentOutput, DaytonaRuntime
    from pydantic_ai import Agent as PydanticAgent
except ImportError:
    # Fallback if agentscaffold is not available
    class AgentInput(BaseModel):
        """Base class for agent inputs."""
        message: str = Field(..., description="Input message for the agent")
        context: Dict[str, Any] = Field(default_factory=dict, description="Additional context")

    class AgentOutput(BaseModel):
        """Base class for agent outputs."""
        response: str = Field(..., description="Response from the agent")
        metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

    class DaytonaRuntime:
        """Minimal DaytonaRuntime implementation."""
        async def execute(self, agent_dir, input_data):
            return {
                "response": f"Daytona execution failed: agentscaffold not available.",
                "metadata": {"error": True}
            }

    class BaseAgent(BaseModel):
        """Minimal base agent implementation."""
        model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")
        
        name: str = "MinimalAgent"
        description: str = ""
        
        def __init__(self, **data):
            super().__init__(**data)
            self._runtime = DaytonaRuntime()
        
        @property
        def runtime(self):
            """Get the DaytonaRuntime instance."""
            return self._runtime
            
        async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """Process input data and generate output."""
            return {"response": f"Processed: {input_data['message']}", "metadata": {}}
        
        async def run(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """Run the agent with the provided input data."""
            if isinstance(input_data, str):
                input_data = {"message": input_data}
                
            # Check if we should force Daytona execution
            force_daytona = input_data.get("context", {}).get("force_daytona", False)
            
            # Process the input
            try:
                if force_daytona and hasattr(self, 'runtime'):
                    # Force Daytona execution
                    import os
                    import inspect
                    
                    # Get the module file path where the agent class is defined
                    agent_module = inspect.getmodule(self.__class__)
                    if agent_module is None:
                        raise RuntimeError("Cannot determine agent module")
                    
                    agent_file = agent_module.__file__
                    if agent_file is None:
                        raise RuntimeError("Cannot determine agent file path")
                    
                    agent_dir = os.path.dirname(os.path.abspath(agent_file))
                    print(f"Forcing Daytona execution. Using agent directory: {agent_dir}")
                    
                    result = await self.runtime.execute(agent_dir, input_data)
                else:
                    # Local execution
                    result = await self.process(input_data)
                
                return result
            except Exception as e:
                print(f"Error running agent: {e}")
                return {"response": f"Error: {e}", "metadata": {"error": True}}
    
    # Fallback minimal PydanticAgent
    class PydanticAgent:
        """Minimal PydanticAgent implementation when pydantic_ai is not available."""
        
        def __init__(self, model_name, result_type=None, system_prompt=None):
            self.model_name = model_name
            self.result_type = result_type
            self.system_prompt = system_prompt
            print(f"Warning: Using fallback PydanticAgent. Install pydantic_ai for full functionality.")
        
        async def run(self, message):
            """Run the agent with the provided message."""
            try:
                # Try to import a direct LLM client based on model_name prefix
                if "openai" in self.model_name:
                    # Try OpenAI
                    import openai
                    client = openai.OpenAI()
                    # Use a default model name if needed
                    model_name = "gpt-4o"
                    if "/" in self.model_name:
                        model_name = self.model_name.split("/", 1)[1]
                    elif ":" in self.model_name:
                        model_name = self.model_name.split(":", 1)[1]
                        
                    response = client.chat.completions.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": self.system_prompt or "You are a helpful assistant."},
                            {"role": "user", "content": message}
                        ]
                    )
                    result_text = response.choices[0].message.content
                elif "anthropic" in self.model_name:
                    # Try Anthropic
                    import anthropic
                    client = anthropic.Anthropic()
                    # Use a default model name if needed
                    model_name = "claude-3-opus-20240229"
                    if "/" in self.model_name:
                        model_name = self.model_name.split("/", 1)[1]
                    elif ":" in self.model_name:
                        model_name = self.model_name.split(":", 1)[1]
                        
                    response = client.messages.create(
                        model=model_name,
                        system=self.system_prompt or "You are a helpful assistant.",
                        messages=[{"role": "user", "content": message}]
                    )
                    result_text = response.content[0].text
                else:
                    # Fallback response
                    result_text = f"I received your message: {message}. However, I'm running in fallback mode without LLM access."
                
                # Create the result object
                result_data = {"message": result_text, "additional_info": {}}
                
                # Create a simple result class
                class MinimalResult:
                    def __init__(self, data):
                        self.data = type('ResultData', (), data)
                
                return MinimalResult(result_data)
            except Exception as e:
                print(f"Error in fallback PydanticAgent: {str(e)}")
                # Even more minimal fallback
                result_data = {"message": f"I received your message, but encountered an error: {str(e)}", "additional_info": {"error": True}}
                class MinimalResult:
                    def __init__(self, data):
                        self.data = type('ResultData', (), data)
                return MinimalResult(result_data)

# Import provider implementations if available
{% if search_provider == "brave" %}
try:
    from agentscaffold.providers.search.brave import BraveSearchProvider
except ImportError:
    # Fallback implementation
    class BraveSearchProvider:
        """Fallback Brave Search Provider implementation."""
        def __init__(self, api_key=None):
            self.api_key = api_key or os.environ.get("BRAVE_API_KEY")
            print("Warning: Using fallback BraveSearchProvider. Install httpx for full functionality.")
            
        def search(self, query, num_results=5, **kwargs):
            """Search implementation."""
            return []
            
        def search_with_snippets(self, query, num_results=3):
            """Generate formatted snippets."""
            return f"No search results available for: {query}"
{% endif %}

{% if memory_provider == "chromadb" %}
try:
    from agentscaffold.providers.memory.chromadb import ChromaDBMemoryProvider
except ImportError:
    # Fallback implementation
    class ChromaDBMemoryProvider:
        """Fallback ChromaDB Memory Provider implementation."""
        def __init__(self, collection_name="agent_memory", **kwargs):
            self.collection_name = collection_name
            print("Warning: Using fallback ChromaDBMemoryProvider. Install chromadb for full functionality.")
            
        def add(self, text, metadata=None):
            """Add implementation."""
            return "fallback-id"
            
        def get_context(self, query, n_results=3, as_string=True):
            """Get context implementation."""
            return "" if as_string else []
{% endif %}

{% if logging_provider == "logfire" %}
try:
    from agentscaffold.providers.logging.logfire import LogFireProvider
except ImportError:
    # Fallback implementation
    class LogFireProvider:
        """Fallback LogFire Provider implementation."""
        def __init__(self, api_key=None, service_name="agent", **kwargs):
            self.api_key = api_key or os.environ.get("LOGFIRE_API_KEY")
            self.service_name = service_name
            self.conversation_id = None
            print("Warning: Using fallback LogFireProvider. Install logfire for full functionality.")
            
        def start_conversation(self, user_id=None):
            """Start a conversation."""
            import uuid
            self.conversation_id = str(uuid.uuid4())
            return self.conversation_id
            
        def log_user_message(self, message, metadata=None):
            """Log a user message."""
            pass
            
        def log_agent_message(self, message, metadata=None):
            """Log an agent message."""
            pass
            
        def log_error(self, error, context=None):
            """Log an error."""
            print(f"Error: {error}")
            
        def end_conversation(self, metadata=None):
            """End a conversation."""
            self.conversation_id = None
{% endif %}


class {{agent_class_name}}Input(AgentInput):
    """Input for {{agent_class_name}} agent."""
    
    # Add custom input fields here
    {% if search_provider != "none" %}
    search_query: Optional[str] = Field(None, description="Optional search query")
    {% endif %}
    {% if memory_provider != "none" %}
    retrieve_context: bool = Field(default=False, description="Whether to retrieve context from memory")
    context_query: Optional[str] = Field(None, description="Optional query for retrieving context")
    store_in_memory: bool = Field(default=False, description="Whether to store the conversation in memory")
    {% endif %}


class {{agent_class_name}}Output(AgentOutput):
    """Output for {{agent_class_name}} agent."""
    
    # Add custom output fields here
    {% if search_provider != "none" %}
    search_results: Optional[List[Dict[str, Any]]] = Field(None, description="Search results if any")
    {% endif %}
    {% if memory_provider != "none" %}
    memory_context: Optional[str] = Field(None, description="Retrieved context if any")
    {% endif %}


class {{agent_class_name}}PydanticResult(BaseModel):
    """Result from Pydantic AI Agent."""
    
    message: str = Field(description="Response message")
    additional_info: Dict[str, Any] = Field(default_factory=dict, description="Additional information")


class Agent(BaseAgent):
    """{{agent_class_name}} agent implementation."""
    
    # Use ConfigDict to set model config with extra="allow" to allow dynamic field assignment
    model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")
    
    name: str = "{{agent_class_name}}"
    description: str = "A {{agent_name}} agent"
    
    # Properly annotate class variables
    input_class: ClassVar[Type[AgentInput]] = {{agent_class_name}}Input
    output_class: ClassVar[Type[AgentOutput]] = {{agent_class_name}}Output
    
    # Include fields for all providers and dynamic components
    pydantic_agent: Optional[Any] = None
    search_provider: Optional[Any] = None
    memory_provider: Optional[Any] = None
    logging_provider: Optional[Any] = None
    
    def __init__(self, **data):
        super().__init__(**data)
            
        # Initialize Pydantic AI agent
        try:
            self.pydantic_agent = PydanticAgent(
                {% if llm_provider == "openai" %}
                "openai:gpt-4o",  # Correct format for pydantic_ai
                {% elif llm_provider == "anthropic" %}
                "anthropic:claude-3-opus-20240229",  # Correct format for pydantic_ai
                {% elif llm_provider == "daytona" %}
                "openai:gpt-4o",  # Fallback to OpenAI
                {% elif llm_provider == "huggingface" %}
                "huggingface/mistralai/Mistral-7B-Instruct-v0.2",  # Correct format for pydantic_ai
                {% elif llm_provider == "ollama" %}
                "openai:gpt-4o",  # Fallback to OpenAI
                {% else %}
                "openai:gpt-4o",  # Fallback to OpenAI
                {% endif %}
                result_type={{agent_class_name}}PydanticResult,
                system_prompt=(
                    "You are {{agent_class_name}}, an AI assistant designed to help with "
                    "{{agent_name}}. Be helpful, concise, and accurate in your responses."
                )
            )
        except Exception as e:
            print(f"Warning: Error initializing PydanticAgent: {str(e)}")
            try:
                # Try a basic OpenAI model as fallback
                self.pydantic_agent = PydanticAgent(
                    "openai:gpt-3.5-turbo",
                    result_type={{agent_class_name}}PydanticResult,
                    system_prompt="You are a helpful assistant."
                )
            except Exception as fallback_error:
                print(f"Warning: Error initializing fallback PydanticAgent: {fallback_error}")
                # Use our custom minimal implementation as last resort
                self.pydantic_agent = PydanticAgent(
                    "openai:gpt-4o",
                    result_type={{agent_class_name}}PydanticResult
                )
        
        # Initialize providers
        {% if search_provider != "none" %}
        # Initialize search provider
        try:
            {% if search_provider == "brave" %}
            self.search_provider = BraveSearchProvider()
            print(f"Initialized Brave Search provider")
            {% endif %}
        except Exception as e:
            print(f"Error initializing search provider: {e}")
            self.search_provider = None
        {% else %}
        self.search_provider = None
        {% endif %}
        
        {% if memory_provider != "none" %}
        # Initialize memory provider
        try:
            {% if memory_provider == "chromadb" %}
            self.memory_provider = ChromaDBMemoryProvider(collection_name="{{agent_name}}_memory")
            print(f"Initialized ChromaDB memory provider")
            {% elif memory_provider == "supabase" %}
            # Existing SupabaseMemoryProvider initialization
            self.memory_provider = self._init_memory()
            {% endif %}
        except Exception as e:
            print(f"Error initializing memory provider: {e}")
            self.memory_provider = None
        {% else %}
        self.memory_provider = None
        {% endif %}
        
        {% if logging_provider != "none" %}
        # Initialize logging provider
        try:
            {% if logging_provider == "logfire" %}
            self.logging_provider = LogFireProvider(service_name="{{agent_name}}")
            print(f"Initialized LogFire logging provider")
            {% endif %}
        except Exception as e:
            print(f"Error initializing logging provider: {e}")
            self.logging_provider = None
        {% else %}
        self.logging_provider = None
        {% endif %}
        
        print(f"Agent initialized: {self.name}")
    
    {% if memory_provider == "supabase" %}
    def _init_memory(self):
        """Initialize memory provider."""
        try:
            from supabase import create_client
            url = os.getenv("SUPABASE_URL")
            key = os.getenv("SUPABASE_KEY")
            if url and key:
                return create_client(url, key)
        except ImportError:
            print("Warning: supabase package not installed. Memory functionality disabled.")
        return None
    
    async def _get_embedding(self, text: str) -> List[float]:
        """Get embedding for text using OpenAI or local alternative."""
        try:
            import openai
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text,
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding error: {str(e)}")
            # Return a zero vector as fallback (not ideal but prevents crashes)
            return [0.0] * 1536
    {% endif %}
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input data and generate output.
        
        Args:
            input_data: Validated input data
            
        Returns:
            Agent output
        """
        try:
            message = input_data["message"]
            context = input_data.get("context", {})
            metadata = {}
            
            # Start logging if provider is available
            if hasattr(self, 'logging_provider') and self.logging_provider:
                if not getattr(self.logging_provider, 'conversation_id', None):
                    self.logging_provider.start_conversation(user_id=context.get("user_id"))
                self.logging_provider.log_user_message(message)
            
            # Handle search if requested
            {% if search_provider != "none" %}
            search_query = input_data.get("search_query") or context.get("search_query")
            if search_query and hasattr(self, 'search_provider') and self.search_provider:
                try:
                    search_results = self.search_provider.search(search_query, num_results=3)
                    metadata["search_results"] = search_results
                    context["search_results"] = self.search_provider.search_with_snippets(search_query)
                except Exception as search_error:
                    print(f"Error performing search: {search_error}")
                    metadata["search_error"] = str(search_error)
            {% endif %}
            
            # Handle memory retrieval if requested
            {% if memory_provider != "none" %}
            retrieve_context = input_data.get("retrieve_context") or context.get("retrieve_context", False)
            context_query = input_data.get("context_query") or context.get("context_query", message)
            if retrieve_context and hasattr(self, 'memory_provider') and self.memory_provider:
                try:
                    memory_context = self.memory_provider.get_context(context_query)
                    context["memory_context"] = memory_context
                    metadata["memory_retrieval"] = True
                except Exception as memory_error:
                    print(f"Error retrieving from memory: {memory_error}")
                    metadata["memory_error"] = str(memory_error)
            {% endif %}
            
            # Build enhanced prompt with context and search results
            enhanced_message = message
            
            # Add retrieved context to the message if available
            if context.get("memory_context"):
                enhanced_message = f"Context from memory:\n{context['memory_context']}\n\nUser message: {message}"
            
            # Add search results to the message if available
            if context.get("search_results"):
                enhanced_message = f"Search results:\n{context['search_results']}\n\nUser message: {message}"
            
            # Process with Pydantic AI
            try:
                result = await self.pydantic_agent.run(enhanced_message)
                response_message = result.data.message
                additional_info = result.data.additional_info
            except Exception as e:
                print(f"Error in pydantic_agent.run: {str(e)}")
                # Fallback response
                response_message = f"I received your message: '{message}'. However, I encountered an error while processing."
                additional_info = {"error": str(e)}
            
            # Store in memory if requested
            {% if memory_provider != "none" %}
            store_in_memory = input_data.get("store_in_memory") or context.get("store_in_memory", False)
            if store_in_memory and hasattr(self, 'memory_provider') and self.memory_provider:
                try:
                    memory_metadata = {
                        "timestamp": time.time(),
                        "source": "conversation",
                        "user_message": message,
                        "agent_response": response_message
                    }
                    entry_id = self.memory_provider.add(f"User: {message}\nAgent: {response_message}", memory_metadata)
                    metadata["memory_entry_id"] = entry_id
                except Exception as memory_error:
                    print(f"Error storing in memory: {memory_error}")
                    metadata["memory_store_error"] = str(memory_error)
            {% endif %}
            
            # Log agent response if logging provider is available
            if hasattr(self, 'logging_provider') and self.logging_provider:
                self.logging_provider.log_agent_message(response_message, metadata=metadata)
            
            # Prepare the output
            output = {
                "response": response_message,
                "metadata": {**additional_info, **metadata}
            }
            
            # Add search_results to output if available
            {% if search_provider != "none" %}
            if "search_results" in metadata:
                output["search_results"] = metadata["search_results"]
            {% endif %}
            
            # Add memory_context to output if available
            {% if memory_provider != "none" %}
            if "memory_context" in context:
                output["memory_context"] = context["memory_context"]
            {% endif %}
            
            return output
            
        except Exception as e:
            # Log the error if logging provider is available
            if hasattr(self, 'logging_provider') and self.logging_provider:
                self.logging_provider.log_error(e, context=input_data)
            
            # Handle any errors
            return {
                "response": f"Error: {str(e)}",
                "metadata": {"error": True, "error_details": str(e)}
            }